{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"./files\")\n",
    "documents = reader.load_data()\n",
    "\n",
    "parser = SentenceSplitter()\n",
    "# nodes = parser.get_nodes_from_documents(documents)\n",
    "# print(vars(nodes[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.91it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.38it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.21it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.12it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.68it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  6.33it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.47it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.01it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "\n",
    "# for jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# in-mem vector DB\n",
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=200, chunk_overlap=0),\n",
    "        TitleExtractor(),\n",
    "        OpenAIEmbedding(),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# Save the cache ( not working for me atm )\n",
    "pipeline.cache.persist(\"./pipeline_cache/llama_cache.json\")\n",
    "pipeline.cache = IngestionCache.from_persist_path(\"./pipeline_cache/llama_cache.json\")\n",
    "\n",
    "nodes = pipeline.run(documents=documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commerce on the internet relies on a trust-based model that involves financial institutions mediating disputes. This model, however, has inherent weaknesses, such as the inability to conduct completely non-reversible transactions and the associated costs of mediation, which can limit the practicality of small transactions and introduce uncertainties. To address these issues, an electronic payment system based on cryptographic proof is proposed, which would allow direct transactions between parties without the need for a trusted third party, thereby protecting sellers from fraud and buyers through routine escrow mechanisms. This system aims to solve the double-spending problem through a peer-to-peer distributed timestamp server that generates computational proof of the chronological order of transactions.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openrouter import OpenRouter\n",
    "import os\n",
    "\n",
    "llm = OpenRouter(\n",
    "    api_key=os.environ['OPENROUTER_API_KEY'],\n",
    "    max_tokens=8192,\n",
    "    context_window=128000,\n",
    "    model=\"qwen/qwen2.5-vl-72b-instruct:free\",\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "response = query_engine.query(\"what does commerce on the internet rely on\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author of the attached paper is not explicitly mentioned in the provided excerpts. The excerpts contain references and bibliographic information but do not include the author's name for the main document.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "response = query_engine.query(\"who is the author of the attached paper?\")\n",
    "print(response)\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "eval_result.passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
